# Results

![Comparisions from Densenet](/densenet.png)


## CIFAR 10
1. 2.72 - [Shake-shake (using resnet)](https://openreview.net/forum?id=HkO-PCmYl&noteId=HkO-PCmYl) [[ torch ](https://github.com/xgastaldi/shake-shake)] 24 Feb 2016
2. 3.40 - [SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE](https://openreview.net/pdf?id=BJYwwY9ll)[[torch](https://github.com/gaohuang/SnapshotEnsemble)] [[keras](https://github.com/titu1994/Snapshot-Ensembles)] 7 Nov 2016
3. 3.58 - [ResNext: Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431) [[torch](https://github.com/facebookresearch/ResNeXt)] [[Chianer](https://github.com/nutszebra/resnext)] [[Caffe](https://github.com/terrychenism/ResNeXt)] [[Tensorflow](https://github.com/wenxinxu/ResNeXt-in-tensorflow)] 16 Nov 2016
4. 3.74 - [SGDR: Stochastic Gradient Descent with Restarts (with wide resnet)](https://arxiv.org/abs/1608.03983)[[Lasange](https://github.com/loshchil/SGDR)] 13 Aug 2016
5. 3.74 - [Densely Connected Convolutional Networks](http://arxiv.org/abs/1608.06993)  [[Caffe Implementation](https://github.com/liuzhuang13/DenseNetCaffe)] [[Torch] (https://github.com/gaohuang/DenseNet_lite)]  [[Tensorflow1] (https://github.com/YixuanLi/densenet-tensorflow)] [[Tensorflow2] (https://github.com/LaurentMazare/deep-models/tree/master/densenet)] [[Tensorflow3] (https://github.com/ikhlestov/vision_networks)] [[Keras1] (https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet)] [[Keras2] (https://github.com/robertomest/convnet-study)] [[Keras3] (https://github.com/titu1994/DenseNet)] [[Chainer2] (https://github.com/t-hanya/chainer-DenseNet)] [[Chainer2] (https://github.com/yasunorikudo/chainer-DenseNet)]  [[PyTorch](https://github.com/andreasveit/densenet-pytorch)] [[PyTorch](https://github.com/bamos/densenet.pytorch)] 25 Aug 2016
6. 3.77 - [Residual Networks of Residual Networks: Multilevel Residual Networks](https://arxiv.org/abs/1608.02908)[[Chainer](https://github.com/nutszebra/residual_networks_of_residual_networks)] 9 Aug 2016
7. 3.80 - [Wide Resnet](http://arxiv.org/abs/1605.07146) [[Torch](https://github.com/szagoruyko/wide-residual-networks)] [[Keras1](https://github.com/asmith26/wide_resnets_keras)] [[Keras2](https://github.com/titu1994/Wide-Residual-Networks)][[TensorFlow](https://github.com/ritchieng/wideresnet-tensorlayer)] 23 May 2016
8. 4.60 - [FractalNet](http://people.cs.uchicago.edu/~larsson/fractalnet/) [[Caffe](https://github.com/gustavla/fractalnet)] [[Keras](https://github.com/snf/keras-fractalnet)] [[Chainer](https://github.com/nutszebra/fractal_net)] May 24 2016
9. 5.23 - [Stochastic Depth Resnets](https://arxiv.org/abs/1603.09382) [[torch](https://github.com/yueatsprograms/Stochastic_Depth)][[Keras](https://github.com/dblN/stochastic_depth_keras)][[Chainer](https://github.com/yasunorikudo/chainer-ResDrop)] 28 Jul 2016
10. 7.72 - [Highway Networks](http://arxiv.org/abs/1507.06228) [[Lasagne](https://github.com/Lasagne/Lasagne/blob/highway_example/examples/Highway%20Networks.ipynb)][[Caffe](https://github.com/flukeskywalker/highway-networks)][[Torch](https://github.com/yoonkim/lstm-char-cnn/blob/master/model/HighwayMLP.lua)][[blog](https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.r2msk226f)] [[Tensorflow](https://github.com/fomorians/highway-cnn)] 23 Nov 2015
11. 3.47 - [Fractional Max-Pooling](http://arxiv.org/abs/1412.6071) [[Notes](https://gist.github.com/shagunsodhani/ccfe3134f46fd3738aa0)][[Theano](https://github.com/diogo149/theano_fractional_max_pooling)] [[Torch layer](https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialFractionalMaxPooling)] [Tensorflow layer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard3/tf.nn.fractional_max_pool.md)] May 13, 2015


### Architectures
* Densenets [[Paper](http://arxiv.org/abs/1608.06993)]  [[Caffe Implementation](https://github.com/liuzhuang13/DenseNetCaffe)] [[Torch] (https://github.com/gaohuang/DenseNet_lite)]  [[Tensorflow1] (https://github.com/YixuanLi/densenet-tensorflow)] [[Tensorflow2] (https://github.com/LaurentMazare/deep-models/tree/master/densenet)] [[Tensorflow3] (https://github.com/ikhlestov/vision_networks)] [[Keras1] (https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/DenseNet)] [[Keras2] (https://github.com/robertomest/convnet-study)] [[Keras3] (https://github.com/titu1994/DenseNet)] [[Chainer2] (https://github.com/t-hanya/chainer-DenseNet)] [[Chainer2] (https://github.com/yasunorikudo/chainer-DenseNet)]  [[PyTorch](https://github.com/andreasveit/densenet-pytorch)] [[PyTorch](https://github.com/bamos/densenet.pytorch)]
  * DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each dense block). For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. This connectivity pattern yields state-of-the-art accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. 
* RESNETS [[Paper](http://arxiv.org/pdf/1512.03385v1.pdf)][[blog](http://torch.ch/blog/2016/02/04/resnets.html)][[Slides](http://kaiminghe.com/icml16tutorial/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf)][[Author github](https://github.com/KaimingHe/deep-residual-networks)][[Keras](https://github.com/fchollet/deep-learning-models)][[TF-Slim](https://github.com/tensorflow/models/tree/master/slim#Pretrained)][[Torch](https://github.com/facebook/fb.resnet.torch)][[PyTorch](https://github.com/pytorch/vision)][[Caffe](https://github.com/BVLC/caffe/wiki/Model-Zoo)]
  * Residual networks add identity skip connnection to layers which makes the layers learn residual transformations. This helps in efficient gradient flow and is currently used in all major network implemetations.
  * VARIATIONS: Stochastic depth[[Torch](https://github.com/yasunorikudo/chainer-ResDrop)][[Keras](https://github.com/dblN/stochastic_depth_keras)][[Chainer](https://github.com/yasunorikudo/chainer-ResDrop)], Wide Residual Networks [[Torch](https://github.com/szagoruyko/wide-residual-networks)][[Keras](https://github.com/asmith26/wide_resnets_keras)], [resnext](https://github.com/facebookresearch/ResNeXt)
* Highway Networks [[Paper1](https://arxiv.org/abs/1607.03474)] [[code](https://github.com/julian121266/RecurrentHighwayNetworks)] [[Paper2](http://arxiv.org/abs/1507.06228)][[Lasagne](https://github.com/Lasagne/Lasagne/blob/highway_example/examples/Highway%20Networks.ipynb)][[Caffe](https://github.com/flukeskywalker/highway-networks)][[Torch](https://github.com/yoonkim/lstm-char-cnn/blob/master/model/HighwayMLP.lua)][[blog](https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.r2msk226f)] [[Tensorflow](https://github.com/fomorians/highway-cnn)]
  * Highway networks, inspired by LSTMs, are a method of constructing networks with hundreds, even thousands, of layers.
* Inception [[paper1](http://arxiv.org/abs/1409.4842v1)][[paper2](http://arxiv.org/abs/1502.03167)][[paper3](http://arxiv.org/abs/1512.00567)][[paper4](http://arxiv.org/abs/1602.07261)][[paper with ResNet combined](http://arxiv.org/abs/1602.07261)][[Tensorflow](https://github.com/tensorflow/models/tree/master/inception)][[TF-slim](https://github.com/tensorflow/models/edit/master/slim/README.md)][[Keras v3](https://github.com/fchollet/deep-learning-models)][[Torch](https://github.com/Moodstocks/inception-v3.torch)][PyTorch]
  * Googles imagenet models using inception architecture that have networks in networks (hence the name). The latest implementation combines inception along with resnets.
* VGG-Net [[Web]](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) [[Paper]](http://arxiv.org/pdf/1409.1556)[[TF-slim](https://github.com/tensorflow/models/edit/master/slim/README.md)][[Keras](https://github.com/fchollet/deep-learning-models)][[Torch](https://github.com/Moodstocks/inception-v3.torch)][[PyTorch](https://github.com/pytorch/vision)]
  * From their website : "Our main contribution is a rigorous evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achieved by increasing the depth to 16-19 weight layers, which is substantially deeper than what has been used in the prior art. To reduce the number of parameters in such very deep networks, we use very small 3Ã—3 filters in all convolutional layers (the convolution stride is set to 1)."
* AlexNet [[Paper]](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012)[[PyTorch](https://github.com/pytorch/vision)][[Torch](https://github.com/eladhoffer/ImageNet-Training)][[TF-Slim](https://github.com/tensorflow/models/blob/master/slim/nets/alexnet.py)][[Torch](https://github.com/soumith/imagenet-multiGPU.torch)]
  * The first deep neural network to prove their performance on Imagenet
  
  
